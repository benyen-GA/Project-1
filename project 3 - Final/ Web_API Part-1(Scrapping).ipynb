{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd57ea5f",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Web APIs & NLP (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eda84b",
   "metadata": {},
   "source": [
    "_________________________________\n",
    "\n",
    "\n",
    "## About the project\n",
    "\n",
    "\n",
    "This project is about applying the concept on webscraping, APIs and Natural Language Processing (LNP).\n",
    "\n",
    "We will be scraping data from Reddit by using Pushshift API, afterwhich applying NLP to model to classify a random post it comes from.\n",
    "_________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a46eb56",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Stocks and real estate investing are both popular investment options for people to grow their their wealth over time.\n",
    "\n",
    "Stocks: When people invest in stocks, they're buying a small piece of ownership in a publicly traded company. The goal is to buy stocks at a low price and sell them at a higher price later on, making a profit. However, stocks can be volatile and unpredictable, with prices fluctuating based on factors like company performance, market conditions, and global events.\n",
    "\n",
    "Real estate investing: This involves buying and managing properties with the goal of earning a profit through rental income, property appreciation, or both. Real estate can be a great long-term investment, but it also requires a significant upfront investment, ongoing maintenance costs, and a knowledge of the local real estate market.\n",
    "\n",
    "\n",
    "As a data analyst for Kabble Securities, I was tasked to develop a machine learning model that can identify key words related to stocks and real estate investing based on user inputs. The challenge is to accurately predict which investments are likely to perform well in the future based on the discussions and patterns observed. The ultimate goal is to provide the company's clients with data-driven insights that will help them make informed investment decisions. The success of this project will depend on the ability of the machine learning model to accurately identify important signals from the noise of online discussions, and the validity of the underlying assumptions used in the model.\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1778b3a5",
   "metadata": {},
   "source": [
    "## Pushshift API\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54e2a7",
   "metadata": {},
   "source": [
    "The scraping data will from `real estate investing` and `stocks` subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55888b21",
   "metadata": {},
   "source": [
    "_______\n",
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "208322dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime as dt \n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b80238",
   "metadata": {},
   "source": [
    "---\n",
    "### Scraping from Reddit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3775ee0f",
   "metadata": {},
   "source": [
    "#### Real estate investing post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10c2135c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, Status code: 200\n",
      "The timestamp is 1676845888\n",
      "The total amount of post is 1000\n",
      "iteration 1, Status code: 200\n",
      "The timestamp is 1675050775\n",
      "The total amount of post is 999\n",
      "iteration 2, Status code: 200\n",
      "The timestamp is 1673453766\n",
      "The total amount of post is 999\n",
      "iteration 3, Status code: 200\n",
      "The timestamp is 1671652051\n",
      "The total amount of post is 999\n",
      "iteration 4, Status code: 200\n",
      "The timestamp is 1669763120\n",
      "The total amount of post is 999\n",
      "iteration 5, Status code: 200\n",
      "The timestamp is 1667596351\n",
      "The total amount of post is 999\n",
      "iteration 6, Status code: 200\n",
      "The timestamp is 1446028231\n",
      "The total amount of post is 1000\n",
      "iteration 7, Status code: 200\n",
      "The timestamp is 1224871583\n",
      "The total amount of post is 445\n"
     ]
    }
   ],
   "source": [
    "#url\n",
    "url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "\n",
    "\n",
    "#Set params\n",
    "params = {\n",
    "     'subreddit': 'realestateinvesting',\n",
    "    'size': 1000}\n",
    "\n",
    "for i in range(8):\n",
    "    if i != 0:\n",
    "        params['before'] = time_stamp\n",
    "    res = requests.get(url, params)\n",
    "    print(f'iteration {i}, Status code: {res.status_code}')\n",
    "    data = res. json()\n",
    "\n",
    "    time_stamp = data['data'][-1]['created_utc']\n",
    "    print(f'The timestamp is {time_stamp}')\n",
    "    posts = data['data']\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame(posts)\n",
    "    else:\n",
    "        df = pd.concat([df, pd.DataFrame(posts)],ignore_index=True, axis = 0)\n",
    "            \n",
    "    print(f'The total amount of post is {len(posts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2b6709c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter out the needs columns from datasets\n",
    "real_estate = df[['subreddit','title','selftext']]\n",
    "#Removing all duplicates\n",
    "real_estate.drop_duplicates(inplace = True)\n",
    "real_estate.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac186d5e",
   "metadata": {},
   "source": [
    "#### Stocks post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dec25382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, Status code: 200\n",
      "The timestamp is 1677158142\n",
      "The total amount of post is 999\n",
      "iteration 1, Status code: 200\n",
      "The timestamp is 1675883410\n",
      "The total amount of post is 1000\n",
      "iteration 2, Status code: 200\n",
      "The timestamp is 1674857190\n",
      "The total amount of post is 1000\n",
      "iteration 3, Status code: 200\n",
      "The timestamp is 1673657171\n",
      "The total amount of post is 1000\n",
      "iteration 4, Status code: 200\n",
      "The timestamp is 1672525682\n",
      "The total amount of post is 1000\n",
      "iteration 5, Status code: 200\n",
      "The timestamp is 1671416827\n",
      "The total amount of post is 1000\n",
      "iteration 6, Status code: 200\n",
      "The timestamp is 1670349892\n",
      "The total amount of post is 999\n",
      "iteration 7, Status code: 200\n",
      "The timestamp is 1669058521\n",
      "The total amount of post is 1000\n",
      "iteration 8, Status code: 200\n",
      "The timestamp is 1667768890\n",
      "The total amount of post is 1000\n",
      "iteration 9, Status code: 200\n",
      "The timestamp is 1471885386\n",
      "The total amount of post is 1000\n",
      "iteration 10, Status code: 200\n",
      "The timestamp is 1469132192\n",
      "The total amount of post is 999\n"
     ]
    }
   ],
   "source": [
    "#url\n",
    "url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "\n",
    "\n",
    "#Set params\n",
    "params = {\n",
    "     'subreddit': 'stocks',\n",
    "    'size': 1000}\n",
    "\n",
    "for i in range(11):\n",
    "    if i != 0:\n",
    "        params['before'] = time_stamp\n",
    "    res = requests.get(url, params)\n",
    "    print(f'iteration {i}, Status code: {res.status_code}')\n",
    "    data = res. json()\n",
    "\n",
    "    time_stamp = data['data'][-1]['created_utc']\n",
    "    print(f'The timestamp is {time_stamp}')\n",
    "    posts = data['data']\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame(posts)\n",
    "    else:\n",
    "        df = pd.concat([df, pd.DataFrame(posts)],ignore_index=True, axis = 0)\n",
    "            \n",
    "    print(f'The total amount of post is {len(posts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bfcb3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter out the needs columns from datasets\n",
    "stocks = df[['subreddit','title','selftext']]\n",
    "#Removing all duplicates\n",
    "stocks.drop_duplicates(inplace = True)\n",
    "stocks.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d759c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Summary of Scraping from Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb47daef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 7287 posts for real estate investing\n",
      "Scraped 10436 posts on for stocks\n",
      "There is 0 duplicates for stocks\n",
      "There is 0 duplicates for real estate investing\n"
     ]
    }
   ],
   "source": [
    "print(f'Scraped {len(real_estate)} posts for real estate investing')\n",
    "print(f'Scraped {len(stocks)} posts on for stocks')\n",
    "print(f'There is {stocks.duplicated().sum()} duplicates for stocks')\n",
    "print(f'There is {real_estate.duplicated().sum()} duplicates for real estate investing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a53d4d",
   "metadata": {},
   "source": [
    "---\n",
    "### Exporting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a4e62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate.to_csv('./datasets/real_estate.csv')\n",
    "stocks.to_csv('./datasets/stocks.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714c42d2",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
